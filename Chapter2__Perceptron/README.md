# 感知机算法
由于本章节重点不在于讲解随机梯度下降算法，所有使用pytorch框架进行有关随机梯度下降算法的实现。运行本章节算法需要安装[pytorch](https://pytorch.org/)
## 算法简述



## 相关问题回答
### 关于书中感知机学习算法的对偶形式
假设按照感知机原始形式所有的参数更新一共需要n次，对偶形式就是把这n次分摊到i个样本中去，这样最终的参数可以展开使用每个样本点进行表示，这样在判断误分类的时候的计算就都可以展开成样本之间的点乘形式，这样就可以通过提前算好的Gram矩阵来大大降低计算量，因为仅仅计算了一次，后续全部通过查表就可以了。而反观原始形式，每次参数改变，所有的矩阵计算全部需要计算，导致计算量比对偶形式要大很多。本质上还是随机梯度下降法。


### 感知机算法与逻辑回归的区别
两者都为线性分类器，只能处理线性可分的数据。两者的损失函数有所不同，PLA针对误分类点到超平面的距离总和进行建模，LR使用交叉熵损失建模。两者的优化方法可以统一为BGD\SGD。LR比PLA的优点之一在于对于激活函数的改进。前者为sigmoid function，后者为step function。LR使得最终结果有了概率解释的能力（将结果限制在0-1之间），sigmoid为平滑函数（连续可导），能够得到更好的分类结果，而step function为分段函数，对于分类的结果处理比较粗糙，非0即1，而不是返回一个分类的概率。而从几何的意义上说，感知机本质上是在参数空间中找到一个分类超平面，而逻辑回归的学习结果